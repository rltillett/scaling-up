---
title: "Scaling Up"
author: "Richard L Tillett"
date: "11/25/2019"
output:
  ioslides_presentation: 
    highlight: haddock
    widescreen: yes
  slidy_presentation:
    highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Scaling up
or
_Going From 1 to N_

## Scaling up
- Doing bioinformatics can mean having **lots** of problems
- it might even be inherent to the whole undertaking
- bioinformatics often means having lots of **_______**

## We have lots of 
- Raw data
- Tools to transform the data
- Dimensions to consider as we try to scale

## An example
- I have 24 paired-end RNA-seq samples in one experiment
- I need to
  - QC 
  - Trimming and More QC
  - Alignment and More QC
  - Gene counting
  - Differential expression analysis
- How do I do the same things to each sample?
- How do I input and run all 24x7 commands efficiently?
```{r, echo=TRUE}
24*7
```

## Some dimensions to consider
- Reproducibility
- Software stack
- Where at?
- CPUs + CPU hours
- Memory (RAM)
- Disk + I/O performance
- Data transfer
- Time + Your time


## Reproducibility and repeatability
- Write it down!
- Low on time? write your history out to a log
- `history > projectName_Date.log`
- Save commands as scripts
- Comment your code for yourself, if for no one else

## Control your software stack
- Know the versions of every tool you use. You'll need them for your methods sections.
- Use version control and Github
- Use Conda (Miniconda) to install bioinformatics tools and fully control the versions you install. 
  - See my [Conda notes](https://github.com/rltillett/conda_notes) to get started with Conda

## Conda pros and cons
- Pro: Install (almost) anything (almost) anywhere. No `sudo` privleiges required.
- Pro: Install exact specific versions of tools.
- Pro: Isolate packages with nasty dependency conflicts into their own environments (e.g. InterproScan)
- Con: Some java tools get renamed by the volunteer who integrated them into conda, making for slight mismatch with official documentation (e.g. trimmomatic)
- Con: If a tool normally comes with optional config files, those are now buried in ugly paths like `~/miniconda3/envs/hisat_env/share/trimmomatic-0.39-1/adapters`
- Con: Frequently doesn't play nice with R/Bioconductor

## Where to scale up?
- Your existing machine, just more efficiently?
- A big workstation?
- A department server?
- HPC clusters (pronghorn)?
- Cloud services?

## Benchmarking (CPU, RAM, Disk)
Where to start?

- Look for documented benchmarks from the authors
- Does your tool have its own internal threading settings?
- Monitor your tool's performance
  - Does it actually use all the cores you give it?
  - Does it secretly use more? `Trimmomatic` often uses 1 more core than I tell it to.
  - Is there an effective limit to its scaling? If running a tool on 8 and 16 cores finishes in nearly the same amount of wall clock time, stick with 8 cores.

## How many CPUs do I have?
Determine how many cores a machine has with `top` and then hit `1`

Mac OS's top doesn't have this, so pull the info with this command:
`system_profiler SPHardwareDataType`

## Time
- As you reduce your time interacting with software, it spends longer running without you present (both proportional and total hours)
- Lots of tools listen to "hangup" signals from the Linux kernel `SIGHUP` that gets sent when you lose your ssh connection
- So you need a way to not log out of a machine when you lose internet or close your laptop
- Use `tmux` (or `screen`) to never lose a connection again

## Scaling up first steps: multitasking
- Backgrounding
- For loops
- Rapid prototyping from explicit commands
- A polling DIY scheduler

## Backgrounding
Use the `&` operator to launch a task into the background and retain your ability to use the command line while it runs. If a program is "chatty," you may be blinded by it's STDOUT and/or STDERR messages, if you don't redirect those to file.

Some examples in bash:

```{bash, eval = FALSE, echo = TRUE}
someProgram & # runs in bkg
someProgram >my.log & # runs in bkg, STDOUT sent to file
someProgram &>my.log & # runs in bkg, STDERR & STDOUT both sent to file
```

## For loops in bash

in bash, `for` loops follow this general form

```{bash, eval = FALSE, echo = TRUE}
for input in a b c d e f
do
  something
  something_else
done
```

## Rapid prototyping with for loops
Quickly go from

- [Explicit commands](https://github.com/rltillett/scaling-up/blob/master/rapid_prototype_explicit_cmds.sh) that ran on one of many sample (from `history`) to
- [A for loop for each of your samples](https://github.com/rltillett/scaling-up/blob/master/rapid_prototype_by_prefix.sh), based directly on it
- using `nano` and the former to make the latter

## For loops an parallelism?

- So, can we do more than one task at a time in a `for` loop with just `&`
- Let's explore this

We'll next make a safe "something" to do that we can test for loops and parallel execution without taxing our computers much

## Making a safe thing for a for loop to "do"
And let's build a shell_script that just runs `sleep` to view how loops stack or don't.
```{bash, eval = FALSE, echo = TRUE}
mkdir testing_loops
cd testing_loops
nano sleepscript.sh
```

And add these two lines.

```{bash, eval = FALSE, echo = TRUE}
#!/bin/bash
sleep 1m
```

Don't forget to `chmod +x sleepscript.sh`

## For loops on patterns of "inputs" 
Let's prep a folder for the task with dummy input files
```{bash, eval = FALSE, echo = TRUE}
touch A{A..M}_R{1,2}.fastq
ls
```

then we build some for loops based off of the patterns and find two parallel problems (live demos)

- demo 1. no backgrounding
- demo 2. using `&` inside the loop. what happens?

## Loops with (iffy) parallel scheduling by polling
- [An example from an interproscan script](https://github.com/rltillett/scaling-up/blob/master/polling_interpro_loop.sh) I recently wrote & used
- Let's examine it
- the inner `while` loop is doing the heavy lifting here
- real schedulers (like HPC have) are preferable, but this kludge does and will work.
- let's adapt it. (back to our terminals again)


## Staying connected w/ terminal multiplexer (tmux)
Before executing a script, invoke `tmux` to avoid hang ups killing your jobs

````{bash, eval = FALSE, echo = TRUE}
tmux
some_script.sh
# [log out by closing your terminal window, AND NOT by typing exit]
```

## More tmux usage
When you log back in

- Use `tmux ls` to list your sessions
- Use `tmux a -t [Number]` to reattach to that session
- Use `tmux detach -t [Number]` to detach a session and make it evaporate

## Oh no, I forgot to tmux!
No guarantees that the following will work, but it is worth a try if you haven't lost connection yet.

- If the program is running in the foreground, CTRL+Z and then bg to background it
- Check your list of running jobs with `jobs`
- Use `disown -h` to remove the most recent job
- Check if it was disowned with `jobs` and repeat `disown -h` as needed

## (Repeat) Where to scale up?
- Your existing machine, just more efficiently?
- A big workstation?
- A department server?
- HPC clusters (pronghorn)?
- Cloud services?


## Added issues on any new system
- Installing software
- Costs?

## Beefy workstations
- Likely good RAM, disk, and OK CPU count
  - Certainly better than a laptop
- You can ask for sudo privlieges
- If you break it, what else is lost, though?
- (Pro? or Con?) it might be a Mac
- Connectivity issues (either "at all" or off-campus access a challenge)

## A department server
- You won't get `sudo`
- Unless a job queue is enabled, users may frequently impact one another
- Use `conda` always

## HPC systems
- Systems like Pronghorn (UNR) & XSEDE (NSF, must apply)
- Huge potential for scaling
- Waiting in lines?
- Job schedulers add complexity and indirectness
- Bad for prototyping (due to job queue)
- You will never get `sudo`
- Use `conda` or containers

## HPC systems
- A few looks at pronghorn and the SLURM job scheduler
- [A different slideshow as pdf](https://github.com/rltillett/scaling-up/blob/master/UNR%20HPC%20resources%20for%20life%20scientists.pdf)
- Q. What makes HPC different than other systems?
  - The job queue. Pronghorn's is called SLURM
  
## HPC job queues
- `srun` and `sbatch` are how you ask permission to run things
- Two scripts to do the job of one (yeah, I know)
- [A launcher script](https://github.com/rltillett/scaling-up/blob/master/trimgalore_dir.sh)
- [A worker script](https://github.com/rltillett/scaling-up/blob/master/trimgalore_worker.sh)
- a look at pronghorn

## Cloud computing
- Free ones
  - [CyVerse Atmosphere, NSF]()
  - Jetstream (NSF, must apply)
  - Amazon AWS (free tier only)
- `$$$` ones
  - Amazon AWS, Google Cloud, Others...
- You will get `sudo`
- You essentially get a bunch of VMs
- Still, use `conda` or containers

## Workflow management systems
- Makefiles
  - compact syntax may be difficult to mentally parse
  - [see this example](http://www.bioinformaticszen.com/post/makefiles/) 
- [Snakemake](https://snakemake.readthedocs.io/en/stable/)
  - Moving as much as I can into snakemake is my next major target.
- [bcbio](https://bcbio-nextgen.readthedocs.io/en/latest/)
  - Has a sense of complexity to it, but is interesting and looks powerful
  
## Containers
- Docker (Cloud)
  - you must have root to build AND run docker images
  - This is generally true on the cloud, though
  - Many many images on [DockerHub](https://hub.docker.com)
- Singularity (Cloud & Pronghorn)
  - root only needed for build and some subsystems
  - Provided the subsystems, you can run anything. Custom or from [Singularity-Hub.org](https://singularity-hub.org)
- Orchestration of containers is Very High Level automation, and a bit out of our scope...

## Thank you

Questions?